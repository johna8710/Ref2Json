# -*- coding: utf-8 -*-
"""Ref2Json

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f_H6HAOXc_9OQ5YP4cN_0utIHPbVWvaD
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install unsloth
# # Also get the latest nightly Unsloth!
# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git

from unsloth import FastVisionModel # FastLanguageModel for LLMs
import torch

model, tokenizer = FastVisionModel.from_pretrained(
    "unsloth/Llama-3.2-11B-Vision-Instruct",
    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for long context
)

from PIL import Image

FastVisionModel.for_inference(model) # Enable for inference!

# Load image from Colab path
image = Image.open("/content/image.jpg")

instruction = "You are an expert radiographer. Describe accurately what you see in this image."

messages = [
    {"role": "user", "content": [
        {"type": "image", "image": image},  # Pass the loaded PIL Image object
        {"type": "text", "text": instruction}
    ]}
]

input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)
inputs = tokenizer(
    image,
    input_text,
    add_special_tokens=False,
    return_tensors="pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt=True)
_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=250,
                   use_cache=True, temperature=1.5, min_p=0.1)

import os
import json
import re
from PIL import Image

def extract_json_from_text(text):
    """Extract JSON array from model output text."""
    # Try to find the JSON array in the text
    # Look for content between [ and ] with nested brackets
    try:
        # Find the position of the first [
        start_pos = text.find('[')
        if start_pos == -1:
            return None

        # Track nested brackets to find the matching closing bracket
        bracket_count = 0
        in_string = False
        escape_char = False

        for i in range(start_pos, len(text)):
            char = text[i]

            # Handle string literals
            if char == '"' and not escape_char:
                in_string = not in_string
            elif char == '\\' and not escape_char:
                escape_char = True
                continue

            if not in_string:
                if char == '[':
                    bracket_count += 1
                elif char == ']':
                    bracket_count -= 1
                    if bracket_count == 0:
                        # Found the matching closing bracket
                        json_str = text[start_pos:i+1]
                        return json.loads(json_str)

            escape_char = False

    except Exception as e:
        print(f"Error extracting JSON: {str(e)}")
        return None

def process_reference_page(image_path, model, tokenizer):
    """Process a single reference page image and return structured citations."""
    try:
        image = Image.open(image_path)
        instruction = """Extract all APA-style references from this image and format them as a JSON array.
        For each reference, include these exact fields: "authors", "year", "title", "journal", "volume", "issue", "pages", "doi".
        Format your response as: [{"authors": [...], "year": "YYYY", ...}, {...}]
        If no references found, respond with: []"""

        messages = [
            {"role": "user", "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": instruction}
            ]}
        ]

        input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)
        inputs = tokenizer(
            image,
            input_text,
            add_special_tokens=False,
            return_tensors="pt",
        ).to("cuda")

        outputs = model.generate(
            **inputs,
            max_new_tokens=5000,
            use_cache=True,
            temperature=0.2,
            min_p=0.1
        )

        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        print("\nRAW MODEL OUTPUT:")
        print("-" * 50)
        print(generated_text)
        print("-" * 50)

        # Extract and parse JSON from the response
        references = extract_json_from_text(generated_text)

        if references:
            print("\nSuccessfully extracted references:")
            print(json.dumps(references, indent=2))
            return references
        else:
            print("\nNo valid JSON array found in output")
            return []

    except Exception as e:
        print(f"Error processing {image_path}: {str(e)}")
        return []

def process_folder(folder_path, model, tokenizer):
    """Process all JPG images in the specified folder."""
    for filename in os.listdir(folder_path):
        if filename.lower().endswith('.jpg'):
            input_path = os.path.join(folder_path, filename)
            output_path = os.path.join(
                folder_path,
                f"{os.path.splitext(filename)[0]}_references.json"
            )

            print(f"\nProcessing {filename}...")
            print("=" * 50)
            references = process_reference_page(input_path, model, tokenizer)

            if references:
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(references, f, indent=2, ensure_ascii=False)
                print(f"\nSaved {len(references)} references to {output_path}")
            else:
                print(f"\nNo valid references extracted from {filename}")
            print("=" * 50)

def main():
    # Enable model for inference
    FastVisionModel.for_inference(model)

    # Process images in specified folder
    input_folder = "/content"  # Update this path as needed
    process_folder(input_folder, model, tokenizer)

if __name__ == "__main__":
    main()

import os
import json
import re
from PIL import Image

def extract_json_objects(text):
    """
    1) Find all occurrences of curly-braced objects: { ... } (greedy, multiline).
    2) For each match, attempt to parse as JSON.
    3) Return a list of all successfully parsed objects.

    This is a fallback approach for when the model outputs multiple JSON objects
    in bullet form, *without* wrapping them in a single [ ... ] array.
    """
    pattern = re.compile(r'\{[^{}]*\}', re.DOTALL)
    matches = pattern.findall(text)

    objects = []
    for match in matches:
        try:
            # Attempt to parse; if valid, accumulate in the list
            parsed_obj = json.loads(match)
            if isinstance(parsed_obj, dict):
                objects.append(parsed_obj)
        except json.JSONDecodeError:
            pass
    return objects

def process_reference_page(image_path, model, tokenizer):
    """Process a single reference page image and return structured citations."""
    try:
        image = Image.open(image_path)

        instruction = """Extract all APA-style references from this image and format them as a JSON array.
        For each reference, include these exact fields: "authors", "year", "title", "journal", "volume", "issue", "pages", "doi".
        Format your response as: [{"authors": [...], "year": "YYYY", ...}, {...}]
        If no references found, respond with: []"""

        messages = [
            {"role": "user", "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": instruction}
            ]}
        ]

        input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)
        inputs = tokenizer(
            image,
            input_text,
            add_special_tokens=False,
            return_tensors="pt",
        ).to("cuda")

        outputs = model.generate(
            **inputs,
            max_new_tokens=5000,
            use_cache=True,
            temperature=0.2,
            min_p=0.1
        )

        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        print("\nRAW MODEL OUTPUT:")
        print("-" * 50)
        print(generated_text)
        print("-" * 50)

        # --- Attempt extraction from a single [ ... ] block first (if any) ---
        references = None
        references = extract_json_from_text(generated_text)  # If you still have this function
        if not references:
            # Fallback: parse any bullet-point JSON objects
            fallback_refs = extract_json_objects(generated_text)
            if fallback_refs:
                references = fallback_refs

        if references:
            print("\nSuccessfully extracted references:")
            print(json.dumps(references, indent=2))
            return references
        else:
            print("\nNo valid JSON array (or objects) found in output")
            return []

    except Exception as e:
        print(f"Error processing {image_path}: {str(e)}")
        return []

def process_folder(folder_path, model, tokenizer):
    """Process all JPG images in the specified folder."""
    for filename in os.listdir(folder_path):
        if filename.lower().endswith('.jpg'):
            input_path = os.path.join(folder_path, filename)
            output_path = os.path.join(
                folder_path,
                f"{os.path.splitext(filename)[0]}_references.json"
            )

            print(f"\nProcessing {filename}...")
            print("=" * 50)
            references = process_reference_page(input_path, model, tokenizer)

            if references:
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(references, f, indent=2, ensure_ascii=False)
                print(f"\nSaved {len(references)} references to {output_path}")
            else:
                print(f"\nNo valid references extracted from {filename}")
            print("=" * 50)

def main():
    # Enable model for inference
    FastVisionModel.for_inference(model)

    # Process images in specified folder
    input_folder = "/content"  # Update this path as needed
    process_folder(input_folder, model, tokenizer)

if __name__ == "__main__":
    main()